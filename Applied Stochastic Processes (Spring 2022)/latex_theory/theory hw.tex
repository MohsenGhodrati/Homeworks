\documentclass{scribe-cgenomics}
\usepackage{float}
\usepackage{subfig}
\usetikzlibrary{matrix, positioning}
\usepackage{mathtools}
\usepackage{tkz-euclide}

\begin{document}

\جلسه
{دکتر کسری علیشاهی، ترم دوم سال تحصیلی ۱۴۰۰}
{تمرین نظری}
{محسن قدرتی}


% ------------------------------------------------------
% problem 1
% ------------------------------------------------------
\newpage
\begin{prob}

\end{prob}

\begin{حل}
 احتمال پیشامد
$n$
بار شیر آمدن در
$2n$
پرتاب یک سکه را
$\theta_n$
بنامیم. داریم:

\begin{center}
$
\theta_n =
\begin{pmatrix}
2n \\ n
\end{pmatrix}
\dfrac{1}{2^{2n}}
= \dfrac{(2n)!}{n!n!2^{2n}}
$
\end{center}

تقریب استرلینگ برای
$n!$
به صورت زیر است:

\begin{center}
$
n! \simeq \sqrt{2\pi n}(\dfrac{n}{e})^{n}
$
\end{center}

در نتیجه

\begin{center}
$
\theta_n \simeq 
\dfrac{\sqrt{2\pi 2n}(\dfrac{2n}{e})^{2n}}{\sqrt{2\pi n}(\dfrac{n}{e})^n \sqrt{2\pi n}(\dfrac{n}{e})^n 2^{2n}}
= \dfrac{1}{\sqrt{\pi n}} \dfrac{2^{2n}n^{2n}e^{-2n}}{2^{2n} n^n e^{-n}n^n e^{-n}}
= \dfrac{1}{\sqrt{\pi n}}
$
\end{center}

پس به ازای
$n=50$،

\begin{center}
$
\theta_{50} \simeq \dfrac{1}{\sqrt{50\pi}} \simeq 0.0797
$
\end{center}

به طور کلی از رابطه بالا نتیجه می‌شود
$\theta_n = 2\theta_{4n}$.
یعنی
$\theta_n$
در حال کاهش است از مرتبه رادیکالی است. که توجیه این موضوع افزایش تعداد حالات تقریبا معادل است. مثلا احتمال
$n-1$
بار شیر آمدن در
$2n$
پرتاب، با
$\theta_n$
در
$n$های
بزرگ تفاوت خاصی ندارد.

\end{حل}



% ------------------------------------------------------
% problem 2
% ------------------------------------------------------
\newpage
\begin{prob}

\end{prob}

\begin{حل}
فرض کنیم در حالت کلی‌تر،
$n$
مهره در ظرف قرار دارد و
$2n$
بار نمونه‌گیری با تکرار کرده‌ایم. اگر متغیر تصادفی
$X_i$
را برابر با تعداد دفعات مشاهده توپ
$i$ام
تعریف کنیم، به دنبال
$E[\delta_{X_1-1} + \delta_{X_2-1} + \dots + \delta_{X_n-1}]$
هستیم. داریم:

\begin{center}
$
E[\delta_{X_1-1} + \delta_{X_2-1} + \dots + \delta_{X_n-1}]
= E[\delta_{X_1-1}] + \dots + E[\delta_{X_n-1}]
$
\end{center}

اما
$X_i$ها
هم‌توزیع هستند و به ازای هر کدام داریم

\begin{center}
$
E[\delta_{X_i-1}] = Pr[X_i = 1]
= \dfrac{2n (n-1)^{2n-1}}{n^{2n}}
= 2(1-\dfrac{1}{n})^{2n-1}
\simeq 2\exp({-\dfrac{2n-1}{n}})
\simeq \dfrac{2}{e^2} 
\simeq 0.27067
$
\end{center}

که در آن، روش شمارش به این ترتیب است که ابتدا مهره
$i$ام
را جایی در دنباله قرار می‌دهیم و سپس بقیه خانه‌های دنباله را از بین بقیه مهره‌ها انتخاب می‌کنیم. پس

\begin{center}
$
E[\delta_{X_1-1} + \delta_{X_2-1} + \dots + \delta_{X_n-1}]
\simeq 2ne^{-2} 
\qquad \implies \qquad
E[\delta_{X_1-1} + \dots + \delta_{X_{100}-1}]
\simeq 27.067
$
\end{center}

\vspace*{1in}

احتمال آنکه همه مهره‌ها دیده شوند (با توجه به هم‌توزیع بودن و تقارن
$X_i$ها)
عبارت است از:

\begin{center}
$
Pr[X_1\geq 1,\ \dots,\ X_n\geq 1]
= 1 - Pr[X_1=0 \cup \dots \cup X_n=0]
$
\bigbreak
$= 1 - nPr[X_1=0]
+\begin{pmatrix}
n\\ 2
\end{pmatrix}
Pr[X_1=0,\ X_2=0] -\dots
$
\bigbreak
$
=
1 - 
\begin{pmatrix}
n\\1
\end{pmatrix}
\dfrac{(2n)^{n-1}}{(2n)^n}
+
\begin{pmatrix}
n\\2
\end{pmatrix}
\dfrac{(2n)^{n-2}}{(2n)^n}
- \dots +
(-1)^n \begin{pmatrix}
n\\n
\end{pmatrix}
\dfrac{(2n)^{n-n}}{(2n)^n}
$
\bigbreak
$
= \sum_{i=0}^{n}(-1)^i \begin{pmatrix}
n\\i
\end{pmatrix}
\dfrac{1}{(2n)^i}
= (1 - \dfrac{1}{2n})^n
\simeq \exp(-\dfrac{n}{2n}) 
= e^{-2} \simeq 0.135
$
\end{center}

پس یک تقریب از احتمال دیده شدن همه گوی‌ها برابر است با
$0.135$.

\end{حل}




% ------------------------------------------------------
% problem 3
% ------------------------------------------------------
\newpage
\begin{prob}
\end{prob}

\begin{حل}

هنگامی که چوب از یک نقطه تصادفی شکسته باشد، اگر طول قطعه بزرگتر را
$L$
بنامیم، خواهیم داشت:

\begin{center}
$
E(L) = 
\int_{0}^{0.5} 1-x dx + \int_{0.5}^{1} xdx
= (x-\dfrac{x^2}{2})\big]_{0}^{0.5} + (\dfrac{x^2}{2})\big]_{0.5}^{1}
= \dfrac{1}{2} - \dfrac{1}{8} + \dfrac{1}{2} - \dfrac{1}{8} 
= \dfrac{3}{4}
$
\end{center}

پس اگر نسبت میانگین طول قطعه بزرگ‌تر ($L$) به میانگین طول قطعه کوچک‌تر ($S$) را بخواهیم، از آنجا که در هر نمونه‌گیری
$L+S=1$
خواهیم داشت:

\begin{center}
$
\dfrac{E(L)}{E(S)} = \dfrac{\frac{3}{4}}{1 - \frac{3}{4}} = 3
\..
$
\end{center}

اما اگر به دنبال
$E(\dfrac{L}{S})$
هستیم، داریم:

\begin{center}
$
\int_{0}^{0.5} \dfrac{1-x}{x} dx + \int_{0.5}^{1} \dfrac{x}{1-x} dx
= \int_{0}^{0.5} \dfrac{1}{x} - 1 dx + \int_{0.5}^{1} \dfrac{1}{1-x} - 1 dx
= (\log x - x)\big]_{0}^{0.5} + (-\log (1-x)-x)\big]_{0.5}^{1}
$
\bigbreak
$
=( -\log 2 - \dfrac{1}{2} - \lim_{x\rightarrow 0^+}\log x)
+ (- \lim_{x\rightarrow 0^+}\log x - 1 -\log 2 + \dfrac{1}{2})
= \infty
$
\end{center}

دلیل اتفاق بالا آن است که مقادیر
$\dfrac{L}{S}$
می‌توانند بدون کران بزرگ باشند و بر اساس نتیجه بالا نیز مشخص است که مقادیر بسیار بزرگ چگالی غیرقابل چشم‌پوشی دارند.

\vspace*{1in}

هنگامی که چوب از دو نقطه تصادفی مستقل و با توزیع یکنواخت شکسته باشد، اگر طول سه قطعه بزرگتر، متوسط و کوچک‌تر را به ترتیب
$L$،
$M$
و
$S$
بنامیم، چون متغیرهای
$X_1$
و
$X_2$
مستقل از یکدیگرند، می‌توانیم فرض کنیم که به طور یکنواخت، از نقاط یک مربع یک در یک انتخاب شده‌اند. برای راحتی بیشتر در کار با متغیر‌ها نیز، فرض کنیم به جای
$X_1$
و
$X_2$
متغیرهای تصادفی مذکور را
$X$
و
$Y$
بنامیم.

\begin{figure}[h]
\begin{tikzpicture}[scale=4]
   \tkzInit[xmax=1,ymax=1,xmin=0,ymin=0]
   \tkzGrid[sub,color=orange,subxstep=.333,subystep=.333]
   \tkzAxeXY
   \draw[-] (0,0) -- (1.2,0.6) node[anchor=west] {$X-2Y=0$};   
   \draw[-] (1,0) -- (0.34,0.66) node[anchor=south] {$X+Y=1$};
   \draw[-] (1,1) -- (0.4,-0.2) node[anchor=north] {$2X-Y=1$};
   \draw[-] (1,1) -- (1,0);
   \draw[-] (0,0) -- (1,1);
  \end{tikzpicture}
  \centering
\end{figure}

مساله دارای تقارن است 
\cite{stick_countor}. 
به این ترتیب که احتمال هر یک از پیشامد‌های زیر برابر با مساحت یک چهارضلعیست که براحتی می‌توان مشاهده کرد برابر با
$\dfrac{1}{6}$
است.

\begin{center}
$
Pr(L=Y) = Pr(L=1-X) = Pr(L=X-Y) = \dots = \dfrac{1}{6}
$
\end{center}

از طرفی توجه می‌کنیم که هر پیشامد
$\{(X,Y)\ |\ X\geq Y,\ L=Y\}$
توسط تناظر‌های داده شده با هر پیشامد زیر متناظر است:

\begin{center}
$
\begin{cases}
(X',\ Y') := (1-Y,\ 1-X) & 
\qquad \{(X',Y')\ |\ X'\geq Y',\ L=1-X'\}\\
(X',\ Y') := (1-X+Y,\ 1-X) & 
\qquad \{(X',Y')\ |\ X'\geq Y',\ L=X'-Y'\}\\
(X',\ Y') := (Y,\ X) & 
\qquad \{(X',Y')\ |\ X'\leq Y',\ L=X'\}\\
(X',\ Y') := (1-X,\ 1-Y) & 
\qquad \{(X',Y')\ |\ X'\leq Y',\ L=1-Y'\}\\
(X',\ Y') := (X-Y,\ X) & 
\qquad \{(X',Y')\ |\ X'\geq Y',\ L=Y'-X'\}\\
\end{cases}
$
\end{center}

پس احتمال‌های شرطی زیر نیز برابرند:

\begin{center}
$
Pr(L\leq l\ |\ L=Y) = Pr(L\leq l\ |\ L=1-X) =Pr(L\leq l\ |\ L=X-Y) = \dots
$
\end{center}

پس طبق قانون احتمال کل داریم:

\begin{center}
$
Pr(L \leq l) = Pr(L\leq l\ |\ L=Y) + \dots + Pr(L\leq l\ |\ L=Y-X) = 6Pr(L\leq l\ |\ L=Y)
$
\end{center}

اما به شرط آنکه طول بزرگترین قطعه برابر با
$Y$
باشد و
$X\geq Y$،
در ناحیه محصور در چهارضلعی به راس‌های
$(1,1)$،
$(1,\dfrac{1}{2})$،
$(\dfrac{2}{3},\dfrac{1}{3})$
و
$(\dfrac{1}{2},\dfrac{1}{2})$
قرار داریم و احتمال شرطی فوق به صورت زیر خواهد بود:

\begin{center}
$
Pr(L\leq l\ |\ L=Y) = 
\begin{cases}
(3l-1)\times \dfrac{l-\frac{1}{3}}{2} &
\qquad \dfrac{1}{3} \leq l \leq \dfrac{1}{2}\\
\dfrac{1}{24} + \dfrac{1}{8} - \dfrac{(1-l)^2}{2} &
\qquad \dfrac{1}{2} \leq l \leq 1
\end{cases}
$
\end{center}

لذا با جایگذاری 
$Pr(L\leq l)$
را خواهیم داشت و از روی تابع توزیع تجمعی، با مشتق‌گیری تابع چگالی
$f_L(l)$
را نیز خواهیم داشت:

\begin{center}
$
Pr(L\leq l) = 
\begin{cases}
(3l-1)^2 &
\qquad \dfrac{1}{3} \leq l \leq \dfrac{1}{2}\\
1 - 3(1-l)^2 &
\qquad \dfrac{1}{2} \leq l \leq 1
\end{cases}
\qquad \implies \qquad
f_L(l) = 
\begin{cases}
6(3l-1) &
\qquad \dfrac{1}{3} \leq l \leq \dfrac{1}{2}\\
6(1-l) &
\qquad \dfrac{1}{2} \leq l \leq 1
\end{cases}
$
\end{center}

که نتیجتا امید ریاضی
$L$
به شکل زیر خواهد بود:

\begin{center}
$
E(L) = 
\int_{\frac{1}{3}}^{\frac{1}{2}} 6l(3l-1)\ dl +
\int_{\frac{1}{2}}^{1} 6l(1-l)\ dl
= \dots = \dfrac{11}{18}
$
\end{center}

با استدلال مشابهی می‌توان نشان داد
$E(S)$
برابر است با شش برابر امید ریاضی
$Y$
به شرطی که مقادیر
$(X,Y)$
محدود به مثلث با اضلاع
$(0,0)$،
$(1,0)$
و
$(\dfrac{2}{3}, \dfrac{1}{3})$
باشند. در این خصوص شکل زیر راهگشاست
\cite{stick_countor}.
این تصویر، سطوح تراز تابع
$f(a,b)$
را نشان می‌دهد.

\begin{center}
$f(a,b) = \min \ \{ \min (a,b),\ \max (a,b)-\min (a,b),\ 1-\max (a,b) \}$
\end{center}

\begin{figure}[h]
\includegraphics[width=1.5in]{3.png}
\centering
\end{figure}

لذا داریم:

\begin{center}
$
E(S)
= 6\Big( \int_{0}^{\frac{2}{3}} \int_{0}^{\frac{x}{2}} y\ dydx +\int_{\frac{2}{3}}^{1} \int_{0}^{1-x} y\ dydx  \Big)
=6\Big( \int_{0}^{\frac{2}{3}} \dfrac{x^2}{8}\ dx  + \int_{\frac{2}{3}}^{1} \dfrac{1}{2} - x + \dfrac{x^2}{2}\ dx\Big)
= \dots = 6\dfrac{1}{54} = \dfrac{1}{9}
$
\end{center}

و از آنجا که
$M = 1-L-S$،
لذا

\begin{center}
$E(M) = E(1-L-S) = 1-\dfrac{11}{18} - \dfrac{1}{9} = \dfrac{5}{18} $
\end{center}


\end{حل}

% ------------------------------------------------------
% problem 4
% ------------------------------------------------------
\newpage
\begin{prob}
\end{prob}

\begin{حل}
متغیر تصادفی
$R_n$
را برابر با تعداد رکوردشکنی‌ها تا لحظه مشاهده
$n$
نمونه در نظر می‌گیریم
\cite{andel}.
فرض کنیم متغیر تصادفی کمکی
$I_i$
عبارت باشد از آنکه متغیر تصادفی
$X_i$
یک رکورد باشد. یعنی

\begin{center}
$
I_i = \begin{cases}
1 & \text{$X_i$ یک رکورد است}\\
0 & \text{در غیر این صورت}
\end{cases}
$
\end{center}

در این صورت احتمال یک بودن
$I_i$
عبارت است از
اینکه در بین مقادیر (با احتمال یک متمایز)
$X_1, \dots, X_i$،
ماکسیمم در جایگاه
$i$ام
قرار بگیرد و بقیه مقادیر با هر ترتیب دلخواهی قرار داشته باشند. یعنی
$Pr[I_i=1] = \dfrac{1}{i}$.

اکنون توجه می‌کنیم که

\begin{center}
$
R_n = \sum_{i=1}^{n} I_i
\implies
E[R_n] = E[I_1 + \dots + I_n] 
= E[I_1] + \dots + E[I_n]
= Pr[I_1=1] + \dots + Pr[I_n=1]
= \sum_{i=1}^{n} \dfrac{1}{i}
$
\end{center}

\vspace*{1in}



\end{حل}


% ------------------------------------------------------
% problem 5
% ------------------------------------------------------
\newpage
\begin{prob}
\end{prob}

\begin{حل}
ابتدا توجه می‌کنیم که با توجه به پارادوکس برتراند، نحوه نمونه‌گیری وترهای دایره می‌تواند متفاوت باشد و به نتایج متفاوتی منجر شود
\cite{bertrand_paradox}.
ما در اینجا فرض می‌کنیم یک وتر توسط میانه‌اش (به طور یکتا) مشخص شود. فرض کنیم دایره اصلی
$2r$
شعاع داشته باشد. در این صورت پیشامد اینکه وتر
$V \in \{(X,Y)\ |\ X^2+Y^2 \leq 4r^2\}$
با دایره کوچک درونی تلاقی داشته باشد، معادل است با اینکه مینیمم فاصله نقاط وتر با مرکز دایره اصلی، فاصله‌ای حداکثر برابر با
$r$
داشته باشد. اما فاصله وتر
$V$
با مرکز دایره اصلی همان فاصله نقطه
$V$
است. پس اگر نقطه میانه وتر را خارج از دایره کوچک انتخاب کنیم، هرگز با دایره کوچک تلاقی نخواهد داشت و برعکس. پس احتمال عدم تلاقی یک وتر تصادفی (نمونه‌گیری شده با روشی که فرض کردیم) عبارت است است از:

\begin{center}
$
\dfrac{\pi 4r^2 - \pi r^2}{\pi 4r^2} = \dfrac{3}{4}
$
\end{center}

در صورتی که دایره بزرگ‌تر با کوچک‌تر هم‌مرکز نباشد، نشان می‌دهیم این احتمال کاهش می‌یابد. ابتدا توجه می‌کنیم که در حالت کلی برای آنکه وتر داده شده
$V \in \{(X,Y)\ |\ X^2+Y^2 \leq 4r^2\}$
با دایره داخلی تلاقی نکند، بایست به ازای دو محل تلاقی دایره کوچک‌تر با خط موازی گذرنده از مرکز دایره کوچک‌تر
($O'$)
با خط
$OV$
(نقاط
$A'$
و
$B'$
در شکل زیر)، 
$V$
بیرون پاهای عمود این دو نقطه بر قطر
$OV$
قرار داشته باشد (بخش‌های سبز شده شکل زیر که به ازای یک قطر خاص با زاویه
$\theta$
با خط
$OO'$
رسم شده‌اند). در این خصوص به شکل زیر توجه می‌کنیم:

\bigbreak

\begin{figure}[h]
\begin{tikzpicture}[x=1cm, y=1cm, scale=1.5]
    \draw[thick] (0,0) circle (2cm) node[anchor=south] {$O$};
    \draw[thick] (0.3,-0.45)  circle (1cm) node[anchor=north] {$O'$};
    \draw[dashed] (0,0) -- (0.3,-0.45);
    \draw[-,color=green] (-2,0) -- (-0.7,0);
    \draw (-2,0) node[anchor=east] {$A$};
    \draw[-,color=gray] (-0.7,0) -- (1.3,0);
    \draw (2,0) node[anchor=west] {$B$};
    \draw[-,color=green] (1.3,0) -- (2,0);
    \draw[-,color=gray] (-0.7,-0.45) -- (1.3,-0.45);
    \draw (-0.7,-0.45) node[anchor=east] {$A'$};
    \draw (1.3,-0.45) node[anchor=west] {$B'$};
    \draw[densely dotted] (-0.7,-0.45) -- (-0.7,0) node[anchor=south] {$V_A$};
    \draw[densely dotted] (1.3,-0.45) -- (1.3,0) node[anchor=south] {$V_B$};
    \draw[] (0.1,-0.15) arc (-56.3:20:0.12);
    \draw (0.1,-0.15) node[anchor=west] {$\theta$};
\end{tikzpicture}
\centering
\end{figure}

آنچه در شکل فوق مورد توجه است، آن است که طول مجموع پاره‌خط سبز، هم‌چنان
$2r$
است، اما شامل دو تکه است که الزاما طول‌های برابر ندارند.

فرض کنیم به ازای
$\theta$
ثابت، طول
$OV_A$
را با
$x_{\theta}$
نشان دهیم. در این صورت مساحت ناحیه شامل همه مراکز وترهای تلاقی کننده با دایره داخلی عبارت است از:

\begin{center}
$
\int_{0}^{\pi} \int_{0}^{2r-x_{\theta}} s\ dsd\theta +
\int_{0}^{\pi} \int_{0}^{x_{\theta}} s\ dsd\theta 
= \int_{0}^{\pi} \dfrac{(2r-x_{\theta})^2}{2} \ d\theta +
\int_{0}^{\pi} \dfrac{x_{\theta}^2}{2}\ d\theta
= \pi r^2 + \int_{0}^{\pi} (r-x_{\theta})^2\ d\theta \geq \pi r^2
$
\end{center}

پس احتمال تلاقی وتر مذکور از
$\dfrac{\pi r^2}{4\pi r^2} = \dfrac{1}{4}$
بیشتر است و لذا احتمال عدم تلاقی کاهش می‌یابد. این کاهش بستگی به میزان
$r-x_{\theta}$
دارد و هرچه دایره کوچک‌تر به سمت مرزهای دایره بزرگ‌تر حرکت کند، احتمال عدم تلاقی وترها کاهش می‌یابد.


\end{حل}













% ------------------------------------------------------
% problem 13
% ------------------------------------------------------
\newpage
\begin{prob}
\end{prob}

\begin{حل}
مساله الکسی ایوانویچ را در حالت کلی که
$n$
واحد پول داشته باشد و با احتمال
$p$
$(p \leq \dfrac{1}{2})$
بتواند نتیجه پرتاب سکه را درست حدس بزند، حل می‌کنیم. پس فرض کنیم الکسی به دنبال برنده شدن مجموعا
$2n$
واحد پول است و از طرفی اگر پولش صفر شود، باید از بازی خارج شود.

نشان خواهیم داد بیشترین شانس الکسی هنگامی رخ می‌دهد که وی همه
$n$
سکه خود را یکجا شرط بندی کند. سپس در حالت کلی‌تر که الکسی به دنبال برنده شدن مجموعا
$n+m$
واحد پول است، پیشنهادی برای بهترین روش عمل الکسی ارایه می‌کنیم.


فرض پیشامد برنده شدن الکسی (دریافت
$n$
واحد پول جدید و خروج از بازی با
$2n$
واحد در مجموع)
را با
$Q_{win}$
نشان دهیم و پیشامد باختن الکسی (از دست
$n$
واحد پول خودش و خروج با
$0$
واحد پول در مجموع) را با
$Q_{lose}$
نشان دهیم. چون تنها اتفاقاتی که منجر به تمام شدن بازی می‌شوند، بردن یا باختن الکسی است، و در زمان متناهی به یکی از این دو اتفاق جاذب در زنجیر مارکف می‌رسیم، پس:

\begin{center}
$
Pr[Q_{win}] + Pr[Q_{lose}] = 1
$
\end{center}

در حالتی که الکسی همه پولش را یکجا شرط‌بندی کند، طبق مفروضات مساله داریم:

\begin{center}
$
Pr[Q_{win}] = p,
\qquad 
Pr[Q_{lose}] = 1-p
\..
$
\end{center}

اکنون نشان می‌دهیم که در صورتی که الکسی هر بار با یک واحد پول شرط‌بندی کند، شانس بردنش کاهش می‌یابد. برای این کار فرض کنیم
$Z_i$
متغیر تصادفی تعداد واحد پول برده یا باخته الکسی در شرط بندی
$i$ام
باشد. در این صورت:

\begin{center}
$
Z_i = 
\begin{cases}
1
&
\text{الکسی با احتمال
$p$
نتیجه پرتاب سکه
$i$ام
را درست پیش‌بینی کرده است.}
\\
-1
&
\text{الکسی با احتمال
$1-p$
نتیجه پرتاب سکه
$i$ام
را اشتباه پیش‌بینی کرده است.}
\end{cases}
$
\end{center}

به علاوه، متغیرهای تصادفی
$S_k = \sum_{i=1}^{k} Z_i$
و
$T$
را به ترتیب برابر با مجموع پول برده یا باخته الکسی تا بازی
$k$ام
و زمان کل بازی تا رخ دادن یکی از پیشامدهای
$Q_{win}$
یا
$Q_{lose}$
در نظر می‌گیریم. در این صورت
$S_T$
شامل دو رخداد بردن یا باختن الکسی است و اگر
$Pr[Q_{win}|T]$
را برابر با
$q_T$
تعریف کنیم، داریم:

\begin{center}
$
Pr[S_T = n] = Pr[Q_{win}|T] = q_T,
\qquad 
Pr[S_T = -n] = Pr[Q_{lose}|T] = 1-q_T
\..
$
\end{center}

اکنون از تساوی والد
\cite{wald_wiki}
نتیجه می‌شود:

\begin{center}
$
E(T)E(Z) = E(S_T) 
\implies 
E(T)
\big( 1\times p + (-1) \times (1-p) \big) 
=
n \times q_T + (-n) \times (1-q_T)
$

$
\implies
E(T) (2p-1) = n(2q_T-1)
$
\end{center}

از طرفی چون در هر گام حداکثر می‌توان یک واحد پول برنده شد، برای برنده شدن
$n$
واحد پول حداقل می‌بایست
$n$
بار بازی کنیم و این یعنی
$E(T) \geq n$.
پس

\begin{center}
$
E(T) \geq n
\implies
E(T)(2p-1) \leq n(2p-1)
\implies
n(2q_T-1) \leq n(2p-1)
\implies q_T \leq p
$
\end{center}

توجه می‌کنیم که در نامساوی بالا، از نامثبت بودن
$2p-1$
یا معادلا از
$p\leq \dfrac{1}{2}$
استفاده شد و اگر
$p\geq \dfrac{1}{2}$،
جهت نامساوی آخر عکس می‌شود. یعنی اگر الکسی قدرت پیش‌بینی نتیجه پرتاب سکه را بهتر از پیش‌بینی تصادفی داشته باشد، بهتر است که به جای یک بازی، بازی‌های یک واحدی انجام دهد و برعکس اگر قدرت پیش‌بینی او (حالت منطقی‌تر) حداکثر تصادفی باشد، بهتر است به یکباره کل سرمایه خود را بازی کند.

\bigbreak

در حالت کلی‌تر زیر نیز نشان می‌دهیم که نتایج بالا هم‌چنان برقرارند. فرض کنیم الکسی در هر بازی خود، تعداد
$r$
واحد پول را با احتمال
$w_r$
بازی کند (در این حالت فرض کرده‌ایم الکسی همواره بتواند تا
$n$
واحد بازی کند، یعنی امکان قرض گرفتن موقت کمی پول را داشته باشد
). در این صورت متغیر تصادفی
$Z_i$
به شکل زیر تعمیم می‌یابد:

\begin{center}
$
Z_i = 
\begin{cases}
n
&
\text{الکسی با احتمال
$w_np$
در بازی
$i$ام
برنده
$n$
واحد پول شده است.}
\\
n-1
&
\text{الکسی با احتمال
$w_{n-1}p$
در بازی
$i$ام
برنده
$n-1$
واحد پول شده است.}
\\
\vdots
\\
-n + 1
&
\text{الکسی با احتمال
$w_{n-1}(1-p)$
در بازی
$i$ام
$n-1$
واحد پول باخته است.}
\\
-n
&
\text{الکسی با احتمال
$w_n(1-p)$
در بازی
$i$ام
$n$
واحد پول باخته است.}
\end{cases}
$
\end{center}

در این صورت مشابه قسمت قبل داریم

\begin{center}
$
E(T)E(Z) = E(S_T) 
\implies
E(T) \big( \sum_{k=1}^{n} (kw_k p + (-k)w_k(1-p)) \big) = n(2q_T-1)
$
\bigbreak
$
\implies
E(T)(2p-1)\sum_{k=1}^{n} kw_k = n(2q_T-1)
$
\end{center}

اما کمیت
$\sum_{k=1}^{n}kw_k$
برابر است با امید ریاضی تعداد واحد پول‌هایی که الکسی در هر بازی شرط‌بندی می‌کند. یا به عبارت دیگر، متوسط پولی که الکسی در هر بازی وارد می‌کند. پس
$E(T)\sum_{k=1}^{n}kw_k$
عبارت است از متوسط پولی که الکسی در مجموع تا رسیدن به
حالت باخت کامل یا برد کامل بازی کرده است و واضح است که این مقدار حداقل برابر با
$n$
است. پس

\begin{center}
$
E(T) \sum_{k=1}^{n}kw_k \geq n
\implies 
E(T) \sum_{k=1}^{n}kw_k (2p-1) \leq n (2p-1)
$

$
\implies 
n(2q_T-1) \leq n(2p-1)
\implies
q_T \leq p
$
\end{center}

توجه می‌کنیم که شرط امکان قرض گرفتن پول توسط الکسی، به نفع او بود و اگر شرط را بر داریم،
$q_T$
کاهش می‌یابد. پس به طور کلی نشان دادیم اگر الکسی قدرت ویژه‌ای در پیش‌بینی نتیجه پرتاب سکه ندارد، بهترین استراتژی او بازی یکباره کل پولش در همان مرحله اول است.

\bigbreak

در درس یادگیری تقویتی دکتر علیشاهی
\cite{alishahi_RL}، 
این مساله بررسی شده است و بهترین استراتژی در آنجا، بازی کردن تعداد واحد پولی بود که هر چه سریع‌تر بازی به اتمام برسد. یعنی رساندن کل پول قمارباز به عددی که پس از آن با ریسک کردن کل پول، هر بار امید به دو برابر شدنش داشته باشیم تا نهایتا کل پولی که به دنبالش هستیم برنده شویم. همانطور که در بالا مشاهده کردیم، نتایج ما با این مشاهدات سازگار هستند.

\end{حل}




% ------------------------------------------------------
% problem 14
% ------------------------------------------------------
\newpage
\begin{prob}
مساله فرآیند پواسون تصادفات جاده‌ای را بررسی می‌کنیم. اگر اطلاعات زیر را داشته باشیم، به دنبال احتمال پیشامد بروز تعدادی تصادف رانندگی در ماه آینده هستیم که هیچ‌کدام مرگبار نباشند.

\begin{center}
$
\begin{cases}
X_i &
\text{پیشامد بروز تصادف
$i$ام
در ماه
}
\\
Y_i &
\text{پیشامد مرگبار بودن تصادف
$i$ام}
\\
N &
\text{متغیر تصادفی تعداد تصادفات رانندگی در یک ماه}
\end{cases}
\implies
\begin{cases}
E(N) = 3\\
Pr[Y_i=1\ |\ X_i=1] = \dfrac{1}{3}
\end{cases}
$
\end{center}
\end{prob}

\begin{حل}

از آنجا که تصادفات جاده‌ای یک فرآیند پواسون است، طبق تعریف، تعداد تصادفات در یک بازه زمانی مشخص (یک ماهه) توزیع پواسون خواهد داشت. از طرفی طبق فرض مساله،
$E(N) = 3$،
و لذا
$N \sim Poisson (3)$.

پس داریم

\begin{center}
$
Pr[N = n] = \dfrac{3^n e^{-3}}{n!}
$
\end{center}

اکنون محاسبات زیر را داریم:

\bigbreak

\begin{LTR}
$
Pr[\text{تعداد مثبتی تصادف رخ دهد و هیچ کدام مرگبار نباشند}]
$
\bigbreak
$
= Pr[\cup_{n=1}^{\infty} \text{$n$ تصادف رخ دهد و هیچ کدام مرگبار نباشند}]
$
\bigbreak
$
= \sum_{n=1}^{\infty} Pr[N=n,\ Y_1=0,\ \dots,\ Y_n=0]
$
\bigbreak
$
= \sum_{n=1}^{\infty} Pr[N=n] Pr[Y_1=0\ |\ X_1=1]\dots Pr[Y_n=0\ |\ X_n=1]
$
\bigbreak
$
= \sum_{n=1}^{\infty} \dfrac{3^n e^{-3}}{n!} (\dfrac{2}{3})^n
$
\bigbreak
$
= e^{-3} \sum_{n=1}^{\infty} \dfrac{2^n}{n!}
$
\bigbreak
$
= e^{-3} (e^2 - 1)
$
\bigbreak
$
= e^{-1} - e^{-3}
$
\end{LTR}

\end{حل}


% ------------------------------------------------------
% problem 15
% ------------------------------------------------------
\newpage
\begin{prob}

\end{prob}

\begin{حل}
متغیر تصادفی
$N(T)$
را برابر با تعداد افرادی تعریف می‌کنیم که اگر
$T$
واحد زمانی طول بکشد تا اتوبوس بعدی برسد، این تعداد از افراد در ایستگاه اتوبوس جمع خواهند شد. به عبارت دیگر،
$N(T)$
تعداد مسافران اتوبوس خواهد بود. توجه می‌کنیم که به دلیل پواسون بودن فرآیندهای زمان رسیدن اتوبوس‌ها و مسافران، متغیر تصادفی
$T$
دارای توزیع نمایی است و متغیر تصادفی
$N(t)$
نیز توزیع پواسون دارد.

\begin{center}
$
T \sim \exp (\dfrac{1}{\mu}),
\qquad
N(t) \sim \text{Poisson}(\lambda)
$
\end{center}

پس

\begin{center}
$
Pr[T=t] = \dfrac{1}{\mu}e^{-\frac{t}{\mu}},
\qquad
Pr[N(t)=n] = \dfrac{(\lambda t)^n e^{-\lambda t}}{n!}
\..
$
\end{center}

ابتدا به ازای یک زمان خاص
$t$،
امید ریاضی تعداد مسافرانی که می‌آیند را حساب می‌کنیم:

\begin{center}
$
E(N(t)\ |\ T=t) 
= \sum_{n=0}^{\infty} n Pr[N(t)=n]
= \sum_{n=0}^{\infty} n\dfrac{(\lambda t)^n e^{-\lambda t}}{n!}
= \sum_{n=1}^{\infty} \dfrac{(\lambda t)^n e^{-\lambda t}}{(n-1)!}
$
\bigbreak
$
= e^{-\lambda t} \lambda t \sum_{n=0}^{\infty} \dfrac{(\lambda t)^n}{n!}
= e^{-\lambda t}\lambda t e^{\lambda t} 
= \lambda t
$
\end{center}

اکنون امید ریاضی
$N(T)$
طبق قضیه امید ریاضی کل، برابر است با:

\begin{center}
$
E(N(T))
= E[E(N(T)\ |\ T)]
= E[\lambda T]
= \lambda \int_{0}^{\infty}t Pr[T=t] dt
= \lambda \int_{0}^{\infty} t \dfrac{1}{\mu} e^{-\frac{t}{\mu}}dt
$
\bigbreak
$
= \lambda (-t e^{-\frac{t}{\mu}})_{0}^{\infty} - \lambda \int_{0}^{\infty} -e^{-\frac{t}{\mu}}dt
= -\lambda (\mu d^{-\frac{t}{\mu}})_{0}^{\infty}
= \lambda \mu
$
\end{center}

\bigbreak

به همین ترتیب،
$var(N(T))$
برابر است با
$E[N(T)^2] - E[N(T)]^2$.
پس به کمک محاسبات زیر سعی می‌کنیم مقدار واریانس را بیابیم.

\begin{center}
$
E(N(t)^2 \ |\ T=t) 
= \sum_{n=0}^{\infty} n^2 Pr[N(t)=n]
= \sum_{n=0}^{\infty} n^2 \dfrac{(\lambda t)^n}{n!}e^{-\lambda t}
= e^{-\lambda t} \sum_{n=1}^{\infty} n \dfrac{(\lambda t)^n}{(n-1)!}
$
\bigbreak
$
= e^{-\lambda t} \lambda t \sum_{n=0}^{\infty} (n+1)\dfrac{(\lambda t)^n}{n!}
= e^{-\lambda t} \lambda t \big( \sum_{n=0}^{\infty} n\dfrac{(\lambda t)^n}{n!} +  \sum_{n=0}^{\infty} \dfrac{(\lambda t)^n}{n!} \big)
= e^{-\lambda t} \lambda t (\lambda t + 1) \sum_{n=0}^{\infty} \dfrac{(\lambda t)^n}{n!}
$
\bigbreak
$
= e^{-\lambda t} \lambda t (\lambda t + 1) e^{\lambda t}
= \lambda t (\lambda t + 1)
$
\end{center}

در نتیجه

\begin{center}
$
var(N(T))
= E[N(T)^2] - E[N(T)]^2
= E[E(N(T)^2\ |\ T)] - (\lambda\mu)^2
= E[\lambda T (\lambda T + 1)] - (\lambda \mu)^2
$
\bigbreak
$
= \lambda^2 E[T^2] + \lambda \mu - (\lambda \mu)^2
$
\end{center}

اما

\begin{center}
$
E[T^2] 
= \int_{0}^{\infty} t^2 \dfrac{1}{\mu} e^{-\frac{t}{\mu}}
= \int_{0}^{\infty} -t^2 d(e^{-\frac{t}{\mu}})
= -t^2e^{-\frac{t}{\mu}}\big]_{0}^{\infty} - \int_{0}^{\infty} -2t e^{-\frac{t}{\mu}}dt
= 2\int_{0}^{\infty}te^{-\frac{t}{\mu}}dt
= 2\mu^2
$
\end{center}

در نتیجه با جایگذاری خواهیم داشت:

\begin{center}
$
var(N(T)) = \lambda^2 2\mu^2 + \lambda \mu - (\lambda \mu)^2 = \lambda \mu (\lambda \mu + 1)
$
\end{center}

در نتیجه میانگین تعداد مسافران هر اتوبوس
$\lambda \mu$
و واریانس آن
$ \lambda \mu (\lambda \mu + 1)$
خواهد بود.

\end{حل}

\newpage
\bibliographystyle{alpha-persian}
\bibliography{main}


\end{document}